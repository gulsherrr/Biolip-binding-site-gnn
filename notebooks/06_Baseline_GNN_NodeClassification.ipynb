{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx-GsKANnm3I",
        "outputId": "04317ca8-ddd1-4822-b9e5-fcd93038ed47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Torch: 2.9.0+cpu CUDA: False\n",
            "PyG: 2.7.0\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "print(\"Torch:\", torch.__version__, \"CUDA:\", torch.cuda.is_available())\n",
        "\n",
        "#Install pytorch geometric (Colab-friendly)\n",
        "!pip -q install torch_geometric\n",
        "\n",
        "import torch_geometric\n",
        "print(\"PyG:\", torch_geometric.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load labeled .npz graph and convert to PyG Data\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/biolip_gnn\")\n",
        "LABELED_DIR = BASE / \"graphs_labeled_v3\"\n",
        "\n",
        "npz_files = sorted(LABELED_DIR.glob(\"*.npz\"))\n",
        "print(\"Labeled graphs found:\", len(npz_files))\n",
        "\n",
        "def load_npz_as_pyg(path: Path) -> Data:\n",
        "  z = np.load(path, allow_pickle = True)\n",
        "\n",
        "  x_idx = torch.tensor(z[\"x_idx\"], dtype=torch.long)\n",
        "  edge_index = torch.tensor(z[\"edge_index\"], dtype=torch.long)\n",
        "  y = torch.tensor(z[\"y\"], dtype=torch.long)\n",
        "\n",
        "  #Optional edge feature -> distance\n",
        "  edge_attr = None\n",
        "  if \"edge_dist\" in z.files:\n",
        "    edge_attr = torch.tensor(z[\"edge_dist\"], dtype=torch.float).view(-1, 1)\n",
        "\n",
        "  data = Data(\n",
        "      x = x_idx.view(-1, 1),\n",
        "      edge_index = edge_index,\n",
        "      edge_attr = edge_attr,\n",
        "      y = y\n",
        "  )\n",
        "  data.pdb_id = str(z[\"pdb_id\"])\n",
        "  data.chain = str(z[\"chain\"])\n",
        "  return data\n",
        "\n",
        "dataset = [load_npz_as_pyg(p) for p in npz_files]\n",
        "print(\"Loaded dataset size:\", len(dataset))\n",
        "print(\"Example:\", dataset[0], dataset[0].pdb_id, dataset[0].chain)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wny9A1xAosMu",
        "outputId": "76909fdf-1fdc-4f04-a954-94cd2eafb6ed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labeled graphs found: 50\n",
            "Loaded dataset size: 50\n",
            "Example: Data(x=[387, 1], edge_index=[2, 4420], edge_attr=[4420, 1], y=[387], pdb_id='1KMM', chain='C') 1KMM C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train/val/test split + class imbalance stats\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "random.shuffle(dataset)\n",
        "\n",
        "n = len(dataset)\n",
        "n_train = int(0.7 * n)\n",
        "n_val = int(0.15 * n)\n",
        "train_set = dataset[:n_train]\n",
        "val_set = dataset[n_train:n_train+n_val]\n",
        "test_set = dataset[n_train+n_val:]\n",
        "\n",
        "print(\"Split:\", len(train_set), len(val_set), len(test_set))\n",
        "\n",
        "def count_labels(ds):\n",
        "  pos = 0\n",
        "  tot = 0\n",
        "  for d in ds:\n",
        "    pos += int(d.y.sum().item())\n",
        "    tot += int(d.y.numel())\n",
        "  return pos, tot\n",
        "\n",
        "pos_train, tot_train = count_labels(train_set)\n",
        "neg_train = tot_train - pos_train\n",
        "print(\"Train positives:\", pos_train, \"Train total:\", tot_train, \"Pos rate:\", pos_train/tot_train)\n",
        "\n",
        "# Weight for BCEWithLogitsLoss: pos_weight = neg/pos\n",
        "pos_weight = torch.tensor([neg_train / max(pos_train, 1)], dtype = torch.float)\n",
        "pos_weight\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwLeH49Zrbh1",
        "outputId": "e2841884-7ecf-4fc0-8257-0834c487c7cf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split: 35 7 8\n",
            "Train positives: 457 Train total: 8721 Pos rate: 0.05240224744868708\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18.0832])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=4, shuffle=True)\n",
        "val_loader   = DataLoader(val_set, batch_size=4, shuffle=False)\n",
        "test_loader  = DataLoader(test_set, batch_size=4, shuffle=False)\n",
        "\n",
        "print(\"Batches:\", len(train_loader), len(val_loader), len(test_loader))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45sOhvOJuwXx",
        "outputId": "c1e86a4d-986a-47db-a0f4-c1b05f65b156"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches: 9 2 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining a tiny baseline GNN for node classification\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "class BaselineSAGE(nn.Module):\n",
        "    def __init__(self, num_aa=21, emb_dim=32, hidden=64):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(num_aa, emb_dim)\n",
        "\n",
        "        self.conv1 = SAGEConv(emb_dim, hidden)\n",
        "        self.conv2 = SAGEConv(hidden, hidden)\n",
        "\n",
        "        self.lin1 = nn.Linear(hidden, hidden)\n",
        "        self.lin2 = nn.Linear(hidden, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x = data.x.squeeze(-1)\n",
        "        x = self.emb(x)\n",
        "\n",
        "        x = self.conv1(x, data.edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, data.edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = F.relu(self.lin1(x))\n",
        "        logits = self.lin2(x).squeeze(-1)\n",
        "        return logits\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3KWAvmMOw21l"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train loop + metric\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BaselineSAGE().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n",
        "\n",
        "def run_epoch(loader, train=False):\n",
        "    model.train() if train else model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    tp = fp = fn = 0\n",
        "\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        with torch.set_grad_enabled(train):\n",
        "            logits = model(batch)\n",
        "            y = batch.y.float()\n",
        "\n",
        "            loss = criterion(logits, y)\n",
        "            if train:\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        probs = torch.sigmoid(logits)\n",
        "        preds = (probs >= 0.5).long()\n",
        "\n",
        "        tp += int(((preds == 1) & (batch.y == 1)).sum().item())\n",
        "        fp += int(((preds == 1) & (batch.y == 0)).sum().item())\n",
        "        fn += int(((preds == 0) & (batch.y == 1)).sum().item())\n",
        "\n",
        "    precision = tp / (tp + fp + 1e-9)\n",
        "    recall    = tp / (tp + fn + 1e-9)\n",
        "    avg_loss  = total_loss / max(len(loader), 1)\n",
        "\n",
        "    return avg_loss, precision, recall\n",
        "\n",
        "EPOCHS = 10\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    tr = run_epoch(train_loader, train=True)\n",
        "    va = run_epoch(val_loader, train=False)\n",
        "    print(f\"Epoch {epoch:02d} | train loss {tr[0]:.4f} P {tr[1]:.3f} R {tr[2]:.3f} \"\n",
        "          f\"| val loss {va[0]:.4f} P {va[1]:.3f} R {va[2]:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhPNy4QZydR5",
        "outputId": "ff987635-e465-4d2e-c891-7ae8651de65d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | train loss 1.3236 P 0.000 R 0.000 | val loss 1.4011 P 0.000 R 0.000\n",
            "Epoch 02 | train loss 1.3252 P 0.088 R 0.098 | val loss 1.3929 P 0.092 R 0.238\n",
            "Epoch 03 | train loss 1.3018 P 0.083 R 0.333 | val loss 1.3875 P 0.091 R 0.343\n",
            "Epoch 04 | train loss 1.2964 P 0.079 R 0.495 | val loss 1.3758 P 0.081 R 0.552\n",
            "Epoch 05 | train loss 1.3113 P 0.084 R 0.455 | val loss 1.3685 P 0.091 R 0.505\n",
            "Epoch 06 | train loss 1.2970 P 0.080 R 0.650 | val loss 1.3526 P 0.073 R 0.771\n",
            "Epoch 07 | train loss 1.2313 P 0.081 R 0.630 | val loss 1.3534 P 0.090 R 0.543\n",
            "Epoch 08 | train loss 1.2331 P 0.088 R 0.600 | val loss 1.3358 P 0.087 R 0.638\n",
            "Epoch 09 | train loss 1.1822 P 0.085 R 0.678 | val loss 1.3378 P 0.092 R 0.581\n",
            "Epoch 10 | train loss 1.2057 P 0.094 R 0.648 | val loss 1.3262 P 0.087 R 0.581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_p, test_r = run_epoch(test_loader, train=False)\n",
        "print(\"TEST | loss:\", round(test_loss,4), \"precision:\", round(test_p,3), \"recall:\", round(test_r,3))\n",
        "\n",
        "SAVE_PATH = BASE / \"out\" / \"day6_baseline_sage.pt\"\n",
        "torch.save(model.state_dict(), SAVE_PATH)\n",
        "print(\"Saved model to:\", SAVE_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wU4vUts51Kou",
        "outputId": "a9863e00-20ad-42e9-e7f5-2045d2b2848e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST | loss: 1.1691 precision: 0.079 recall: 0.604\n",
            "Saved model to: /content/drive/MyDrive/biolip_gnn/out/day6_baseline_sage.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JzRKj2Rj16RS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}